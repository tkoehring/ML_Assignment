# ML Assignment
## Task 2
To accomdate the multi-task learning expansion, I added two seperate heads to the model, one for each of the different tasks. In this case the two tasks I went with were sentence classification specifically classifying on emotions and sentiment analysis. Therefore for the classification head I created a new fully connected layer with the dimensions matching the size of the hidden layer in the pre-trained model by 6, the number of emotions from the dataset I chose to work with. For the sentiment analysis head it follows suit with the hidden layer and the second dimension is 2, for pos/neg labels.

For the forward function of the model I opted to add in the notion of "task id", which would determine which head would be selected while training. This allows for the model to be trained in such a way that we can interweave the two different tasks without skewing the model to one task entirely, then the other causing the model to potentially "forget" what it learned about the first task.

## Task 3
* Freezing the entire network can be benficial to keep the information the pre-trained model had learned about language in the general sense while still being fine tuned on the specific tasks at had. This would hold true moreso when the training data for the specific tasks is small.

* Similar to freezing the entire network, freezing just the transformer back-bone still allows for the model to retain it's general understanding of language but now can continue to become more refined towards each of the tasks added on top of the base model. In this case we would need a sufficient amount of data to train both tasks of the model so they the unfrozen layers can make meaningful progress to adapating to our new tasks as well as our task specific heads. Similar to how I described my training methodolgy in part 2, I would interweave the different tasks training data in mini-batches to prevent the model from heavily skewing towards one task, then losing that information gain by heavily skewing towards another task.

* By freezing a task specific head, we would have to be under the presumption that the one we are freezing has already been trained to the point of satisfaction such that we don't need to worry about modifying it and want to preserve its weights. In terms of training we would simply training only on the unfrozen tasks data to get it to a point that we are happy with.

* Transfer learning can be beneficial in that it can increase our efficiency in generating a new model, especially one that is of a more specific task. Models such as the one I chose, "distilbert", are a bit more general in their training than something specifically designed for say programming auto-completes. Additionally using something like "distilbert" is beneficial for another reason, that it was designed to be a "smaller" model without sacrificing too much accuracy, which is fine tuning and optimizing we no longer need to worry about. I would freeze the transformer back-bone to not only help speed up the process but due to the fact that I do not have the large amount of data that the BERT model was trained on to really improve the transformer model in any meaningful way. With BERT and by associciation "distilbert" being trained on Book Corpus and Wikipedia, I am much better of using my data to fine tune later layers of the model and the task specific heads to hone in on the accuracy of my new learning tasks. In terms of freezing the task specific heads, I would keep the frozen and supply data in such a way that no one task is too heavily favored.

## Task 4
For the training loop I decided to use the cross-entropy loss functions since both of my datasets involved classification and for metrics I went with accuracy. During training I would pass a batch of task one data through the model, then a batch of task 2 data. This seemed like a good strategy to avoid Catastrophic Forrgetting, and not having the model heavily skewed towards one task vs another. This would be even more pronounced if I had more unfrozen layers. Given more time I would like to explore the training parameter space (learning rate, early stopping, batch size) and other metrics for improved accuracy. Additionally potentially adding another layer between the transformer backbone and the new task heads or unfreezing some of the transformer, to allow for more specific knowledge gain. 
